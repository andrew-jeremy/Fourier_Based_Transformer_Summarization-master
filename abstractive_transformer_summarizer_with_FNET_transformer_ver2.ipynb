{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "abstractive_transformer_summarizer_with FNET_transformer_ver2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n9YqvqJ4WwbV"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pSpFiCvVmqJ"
      },
      "source": [
        "**Removed hidden layers to create a single Fourier Transformer - one hidden layer**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faSguIGBXJvf",
        "outputId": "d65abf68-a242-4dc1-bdb9-d2e1b0155c9c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STHLg5InXPG4",
        "outputId": "787a630b-1679-487e-e235-ee4d618cd86a"
      },
      "source": [
        "%cd /content/drive/My Drive/Intelligence Report Summarization/abstractive_summarizer-master/Transformer-networks-for-abstractive-summarization-master\n",
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Intelligence Report Summarization/abstractive_summarizer-master/Transformer-networks-for-abstractive-summarization-master\n",
            "checkpoints4  glove.6B.200d.txt  README.md   train_dataset.csv\n",
            "files\t      main-github.ipynb  scores.pkl  val_dataset.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ3_5ufNkcH4"
      },
      "source": [
        "%%capture\n",
        "!pip install datasets==1.2.1\n",
        "!pip install rouge_score\n",
        "!pip install rouge"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O41YiFHFWwbG"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import unicodedata\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as krs\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer \n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from datasets import load_metric\n",
        "import csv\n",
        "from rouge import Rouge\n",
        "\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 4 #64\n",
        "embedding_dim = 200 "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zDqf0iXkuuF"
      },
      "source": [
        "saved = False # True to load data fresh otherwise load from saved files\n",
        "if saved == True:\n",
        "   from datasets import load_dataset, load_metric\n",
        "   train_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"train\")\n",
        "   val_dataset = load_dataset(\"scientific_papers\", \"pubmed\", split=\"validation\")\n",
        "\n",
        "   df_train = pd.DataFrame(train_dataset)\n",
        "   df_train.to_csv(\"train_dataset.csv\", sep='\\t')\n",
        "   df_valid = pd.DataFrame(val_dataset)\n",
        "   df_valid.to_csv(\"val_dataset.csv\", sep='\\t')\n",
        "   df_train.head()\n",
        "else:  # Read from Saved Files\n",
        "  df_train = pd.read_csv(\"train_dataset.csv\", sep='\\t')\n",
        "  df_valid = pd.read_csv(\"val_dataset.csv\", sep='\\t')\n",
        "  summaries, longreview = pd.DataFrame(), pd.DataFrame()\n",
        "  summaries['short'] = df_train['abstract'].head(1000)\n",
        "  longreview['long'] = df_train['article'].head(1000)\n",
        "  (summaries.shape,longreview.shape)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PODBOKO8WwbK"
      },
      "source": [
        "### Cleaning the data for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKOiIlB9WwbK"
      },
      "source": [
        "# replacing many abbreviations and lower casing the words\n",
        "def clean_words(sentence):\n",
        "    sentence = str(sentence).lower()\n",
        "    sentence = unicodedata.normalize('NFKD', sentence).encode('ascii', 'ignore').decode('utf-8', 'ignore') # for converting Ã© to e and other accented chars\n",
        "    sentence = re.sub(r\"http\\S+\",\"\",sentence)\n",
        "    sentence = re.sub(r\"there's\", \"there is\", sentence)\n",
        "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
        "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
        "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
        "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
        "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
        "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
        "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
        "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
        "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
        "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
        "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
        "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
        "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
        "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
        "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
        "    sentence = re.sub(r\"'til\", \"until\", sentence)\n",
        "    sentence = re.sub(r\"\\\"\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\'\", \"\", sentence)\n",
        "    sentence = re.sub(r' s ', \"\",sentence)\n",
        "    sentence = re.sub(r\"&39\", \"\", sentence) # the inshorts data has this in it\n",
        "    sentence = re.sub(r\"&34\", \"\", sentence) # the inshorts data has this in it\n",
        "    sentence = re.sub(r\"[\\[\\]\\\\0-9()\\\"$#%/@;:<>{}`+=~|.!?,-]\", \"\", sentence)\n",
        "    sentence = re.sub(r\"&\", \"\", sentence)\n",
        "    sentence = re.sub(r\"\\\\n\", \"\", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGZ_N1QoWwbL"
      },
      "source": [
        "summaries['short'] = summaries['short'].apply(lambda x: clean_words(x))\n",
        "longreview['long'] = longreview['long'].apply(lambda x: clean_words(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Xo_-edOwWwbM",
        "outputId": "ebe5bcc9-d9a7-4f7d-d16f-5a418bb5cf29"
      },
      "source": [
        "longreview.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a recent systematic analysis showed that in   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it occurs in more than  of patients and may re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tardive dystonia  td   a rarer side effect aft...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lepidoptera include agricultural pests that  t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>syncope is caused by transient diffuse cerebra...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                long\n",
              "0  a recent systematic analysis showed that in   ...\n",
              "1  it occurs in more than  of patients and may re...\n",
              "2  tardive dystonia  td   a rarer side effect aft...\n",
              "3  lepidoptera include agricultural pests that  t...\n",
              "4  syncope is caused by transient diffuse cerebra..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7901FCN2WwbM",
        "outputId": "65b25202-147f-460d-a28f-1739137caf9b"
      },
      "source": [
        "# adding start and end token to the sentences of label \n",
        "start_token, end_token = '<startseq>' , '<endseq>'\n",
        "summaries = summaries.apply(lambda x: start_token + ' ' + x + ' ' + end_token)\n",
        "summaries.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>short</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;startseq&gt; background  the present study was c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;startseq&gt; backgroundanemia in patients with c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;startseq&gt; tardive dystonia  td  is a serious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;startseq&gt; many lepidopteran insects are agric...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;startseq&gt; we present an unusual case of recur...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               short\n",
              "0  <startseq> background  the present study was c...\n",
              "1  <startseq> backgroundanemia in patients with c...\n",
              "2  <startseq> tardive dystonia  td  is a serious ...\n",
              "3  <startseq> many lepidopteran insects are agric...\n",
              "4  <startseq> we present an unusual case of recur..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLTNXQeYWwbN",
        "outputId": "1768e52e-9ffa-419e-f224-6c9319c2cfc8"
      },
      "source": [
        "val_split = 0.2\n",
        "# train validation split\n",
        "summaries_train = summaries[int(len(summaries)*val_split):]\n",
        "summaries_val = summaries[:int(len(summaries)*val_split)]\n",
        "longreview_train = longreview[int(len(summaries)*val_split):]\n",
        "longreview_val = longreview[:int(len(summaries)*val_split)]\n",
        "\n",
        "len(longreview_val),len(longreview_train)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 800)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HshA0SDlWwbN",
        "outputId": "110b0c0d-d8da-4d15-85ce-fcf603d83a10"
      },
      "source": [
        "longreview_train.iloc[0], summaries_train.iloc[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(long    tuberculosis  tb  is caused by the mycobacteri...\n",
              " Name: 200, dtype: object,\n",
              " short    <startseq> background  tuberculosis  tb  remai...\n",
              " Name: 200, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzzfpqEZWwbO"
      },
      "source": [
        "finding the maximum length of questions and answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMdGM1IFWwbO",
        "outputId": "62719716-8a68-4a8f-8c93-422b6e6a0ac9"
      },
      "source": [
        "# because there are senteces with unusually long lengths, \n",
        "# we caculate the max length that 95% of sentences are shorter than that\n",
        "def max_length(shorts, longs, prct):\n",
        "    # Create a list of all the captions\n",
        "    \n",
        "    length_longs = list(len(d.split()) for d in longs)\n",
        "    length_shorts = list(len(d.split()) for d in shorts)\n",
        "\n",
        "    print('percentile {} of length of news: {}'.format(prct,np.percentile(length_longs, prct)))\n",
        "    print('longest sentence: ', max(length_longs))\n",
        "    print()\n",
        "    print('percentile {} of length of summaries: {}'.format(prct,np.percentile(length_shorts, prct)))\n",
        "    print('longest sentence: ', max(length_shorts))\n",
        "    print()\n",
        "    return int(np.percentile(length_longs, prct)),int(np.percentile(length_shorts, prct))\n",
        "\n",
        "# selecting sentence length based on the percentile of data that fits in the length\n",
        "max_len_news, max_len_summary= max_length(summaries_train['short'].to_list(), longreview_train['long'].to_list(), 90)\n",
        "\n",
        "\n",
        "print('max-length longreview chosen for training: ', max_len_news)\n",
        "print('max-length summaries chosen for training: ', max_len_summary)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "percentile 90 of length of news: 4553.5\n",
            "longest sentence:  18327\n",
            "\n",
            "percentile 90 of length of summaries: 247.0\n",
            "longest sentence:  333\n",
            "\n",
            "max-length longreview chosen for training:  4553\n",
            "max-length summaries chosen for training:  247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhE2xTwUWwbP"
      },
      "source": [
        "### Dataset prepration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFQZYqmKWwbP"
      },
      "source": [
        "# making a vocabulary of the words \n",
        "def create_vocab(shorts, longs = None, minimum_repeat = 3):\n",
        "\n",
        "    # Create a list of all the captions\n",
        "    all_captions = []\n",
        "    for s in shorts:\n",
        "        all_captions.append(s)\n",
        "\n",
        "    # Consider only words which occur at least minimum_occurrence times in the corpus\n",
        "    word_counts = {}\n",
        "    nsents = 0\n",
        "    for sent in all_captions:\n",
        "        nsents += 1\n",
        "        for w in sent.split(' '):\n",
        "            word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "    vocab = [w for w in word_counts if word_counts[w] >= minimum_repeat]\n",
        "    \n",
        "    vocab = list(set(vocab))\n",
        "    return vocab"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfYWBPsHWwbQ",
        "outputId": "6f97338d-f259-4cb8-f075-e8cd368ec0a5"
      },
      "source": [
        "# each word in the vocabulary must be used in the data atleast minimum_repeat times\n",
        "vocab_dec = create_vocab(summaries_train['short'].to_list(), minimum_repeat=5) # here we just use the words in vocabulary of summaries\n",
        "# removing one character words from vocab except for 'a'\n",
        "for v in vocab_dec:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        vocab_dec.remove(v) \n",
        "        \n",
        "vocab_dec = sorted(vocab_dec)[1:] # [1:] is for the '' \n",
        "vocab_dec[:10]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<endseq>',\n",
              " '<startseq>',\n",
              " 'a',\n",
              " 'aa',\n",
              " 'aaa',\n",
              " 'aabs',\n",
              " 'aac',\n",
              " 'aass',\n",
              " 'abdomen',\n",
              " 'abdominal']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "silZ-w0qWwbQ",
        "outputId": "64148d6a-204f-463c-ed50-5436b4aba023"
      },
      "source": [
        "# each word in the vocabulary must be used in the data atleast minimum_repeat times\n",
        "vocab_enc = create_vocab(longreview_train['long'].to_list(), minimum_repeat=3) # here we just use the words in vocabulary of summaries\n",
        "# removing one character words from vocab except for 'a'\n",
        "for v in vocab_enc:\n",
        "    if len(v) == 1 and v!='a' and v!='i':\n",
        "        vocab_enc.remove(v) \n",
        "        \n",
        "vocab_enc = sorted(vocab_enc)[1:] # [1:] is for the '' \n",
        "vocab_enc[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n\\t\\t\\t\\t',\n",
              " '\\n\\n',\n",
              " '\\n\\n\\n',\n",
              " '\\n\\n\\n\\n',\n",
              " '\\n\\n\\npersonal',\n",
              " '\\n\\n\\nthe',\n",
              " '\\n\\na',\n",
              " '\\n\\nadditionally',\n",
              " '\\n\\nall',\n",
              " '\\n\\nan']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebYY3mNVWwbR",
        "outputId": "196bb057-a3b9-4812-e1ab-208788ae38b7"
      },
      "source": [
        "oov_token = '<UNK>'\n",
        "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n' # making sure all the last non digit non alphabet chars are removed\n",
        "document_tokenizer = krs.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\n",
        "summary_tokenizer = krs.preprocessing.text.Tokenizer(filters = filters,oov_token=oov_token)\n",
        "document_tokenizer.fit_on_texts(vocab_enc)\n",
        "summary_tokenizer.fit_on_texts(vocab_dec)#summaries_train['short'])\n",
        "\n",
        "# caculating number of words in vocabulary of encoder and decoder\n",
        "# they are important for positional encoding\n",
        "encoder_vocab_size = len(document_tokenizer.word_index) + 1 \n",
        "decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n",
        "\n",
        "# vocab_size\n",
        "encoder_vocab_size, decoder_vocab_size"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23936, 3761)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeiWLGG9WwbR"
      },
      "source": [
        "ixtoword_enc = {} # index to word dic\n",
        "ixtoword_dec = {} # index to word dic\n",
        "\n",
        "wordtoix_enc = document_tokenizer.word_index # word to index dic\n",
        "ixtoword_enc[0] = '<PAD0>' # no word in vocab has index 0. but padding is indicated with 0\n",
        "ixtoword_dec[0] = '<PAD0>' # no word in vocab has index 0. but padding is indicated with 0\n",
        "\n",
        "for w in document_tokenizer.word_index:\n",
        "    ixtoword_enc[document_tokenizer.word_index[w]] = w\n",
        "################################################\n",
        "wordtoix_dec = summary_tokenizer.word_index # word to index dic\n",
        "\n",
        "for w in summary_tokenizer.word_index:\n",
        "    ixtoword_dec[summary_tokenizer.word_index[w]] = w"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhEE8B3FWwbR"
      },
      "source": [
        "# assign a number to each word inorder to find it in word embeddings\n",
        "inputs = document_tokenizer.texts_to_sequences(longreview_train['long'])\n",
        "targets = summary_tokenizer.texts_to_sequences(summaries_train['short'])\n",
        "inputs_val = document_tokenizer.texts_to_sequences(longreview_val['long'])\n",
        "targets_val = summary_tokenizer.texts_to_sequences(summaries_val['short'])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww8j6p2uWwbS"
      },
      "source": [
        "inputs = krs.preprocessing.sequence.pad_sequences(inputs, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets = krs.preprocessing.sequence.pad_sequences(targets, maxlen=max_len_summary, padding='post', truncating='post')\n",
        "inputs_val = krs.preprocessing.sequence.pad_sequences(inputs_val, maxlen=max_len_news, padding='post', truncating='post')\n",
        "targets_val = krs.preprocessing.sequence.pad_sequences(targets_val, maxlen=max_len_summary, padding='post', truncating='post')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsx0ryqjWwbS"
      },
      "source": [
        "# validate train split\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs,targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset_val = tf.data.Dataset.from_tensor_slices((inputs_val,targets_val)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE*2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC46PK_gWwbS"
      },
      "source": [
        "longreview_val.reset_index(inplace=True, drop=True)\n",
        "summaries_val.reset_index(inplace=True, drop=True)\n",
        "longreview_train.reset_index(inplace=True, drop=True)\n",
        "summaries_train.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOr2S9DLWwbS"
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr-TTLRQWwbT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hist(history):\n",
        "    plt.title('Loss')\n",
        "\n",
        "    x= [i[0] for i in history['val']]\n",
        "    y=[i[1] for i in history['val']]\n",
        "    plt.plot(x,y,'x-')\n",
        "    \n",
        "    x= [i[0] for i in history['train']]\n",
        "    y=[i[1] for i in history['train']]    \n",
        "    plt.plot(x,y,'o-')\n",
        "\n",
        "    plt.legend(['validation','train'])\n",
        "    plt.show()\n",
        "    print('smallest val loss:', sorted(history['val'],key=lambda x: x[1])[0])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBhD_2yQWwbT"
      },
      "source": [
        "#### Scaled Dot Product\n",
        "![](files/z-score.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbId0hZpR0Qf"
      },
      "source": [
        "import jax\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def fftn(x):\n",
        "  \"\"\"Applies n-dimensional Fast Fourier Transform (FFT) to input array.\n",
        "  This function reproduces the JAX native n-dimensional FFT, jax.numpy.fftn.\n",
        "  For the special case of 2D FFTs, we have found that applying 1D FFTs is\n",
        "  faster, on TPUs, than applying the native JAX implementation.\n",
        "  TODO(b/181607810): Revisit optimization as XLA FFT implementation improves.\n",
        "  Args:\n",
        "    x: Input n-dimensional array.\n",
        "  Returns:\n",
        "    n-dimensional Fourier transform of input n-dimensional array.\n",
        "  \"\"\"\n",
        "  out = tf.dtypes.cast(x,tf.complex64)\n",
        "  out = tf.signal.fft2d(out)\n",
        "  out = tf.math.real(out)\n",
        "  return out"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0966fNEWwbU"
      },
      "source": [
        "#### Fourier based Multi-Headed Attention\n",
        "![](files/multi-head.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OseAD5-LuW7t"
      },
      "source": [
        "class FourierAttention(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(FourierAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "    def call(self, fourier, out, mask):\n",
        "        batch_size = tf.shape(out)[0]\n",
        "        \n",
        "        # reshape them\n",
        "        fourier = self.split_heads(fourier, batch_size)\n",
        "        out = self.split_heads(out, batch_size)\n",
        "  \n",
        "        matmul_qk = tf.matmul(out,fourier, transpose_b=True)\n",
        "        output = tf.matmul(matmul_qk, fourier)\n",
        "        scaled_output = tf.transpose(output, perm=[0, 2, 1, 3])\n",
        "        \n",
        "        # the last dens layer expect one vector so we use concat\n",
        "        concat_attention = tf.reshape(scaled_output, (batch_size, -1, self.d_model))\n",
        "        output = concat_attention\n",
        "        return output"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3s6smfAl-du"
      },
      "source": [
        "test_a = tf.constant([[1, 2], [3, 4]])\n",
        "test_b = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "def pad_second_dim(input, desired_size):\n",
        "    padding = tf.tile([[0]], tf.stack([tf.shape(input)[0], desired_size - tf.shape(input)[1]], 0))\n",
        "    return tf.concat([input, padding], 1)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mexm4ZESWwbU"
      },
      "source": [
        "#### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfC90iW8WwbU"
      },
      "source": [
        "def get_angles(position, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return position * angle_rates\n",
        "\n",
        "# The dimension of positional encodings is the same as\n",
        "# the embeddings (d_model) for facilitating the summation of both.\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(\n",
        "        np.arange(position)[:, np.newaxis],\n",
        "        np.arange(d_model)[np.newaxis, :],\n",
        "        d_model\n",
        "    )\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9YqvqJ4WwbV"
      },
      "source": [
        "#### Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvPQjH0NWwbV"
      },
      "source": [
        "#### Embeddings preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBRI40LGWwbV",
        "outputId": "1786b2a3-c797-4739-d6fa-cd0fe7cb589e"
      },
      "source": [
        " # Making the embedding mtrix\n",
        "def make_embedding_layer(vocab_len, wordtoix, embedding_dim=200, glove=True, glove_path= '../glove'):\n",
        "    if glove == False:\n",
        "        print('Just a zero matrix loaded')\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # just a zero matrix \n",
        "    else:\n",
        "        print('Loading glove...')\n",
        "        glove_dir = glove_path\n",
        "        embeddings_index = {} \n",
        "        f = open(\"glove.6B.200d.txt\", encoding=\"utf-8\")\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        # Get n-dim dense vector for each of the vocab_rocc\n",
        "        embedding_matrix = np.zeros((vocab_len, embedding_dim)) # to import as weights for Keras Embedding layer\n",
        "        for word, i in wordtoix.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # Words not found in the embedding index will be all zeros\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "        print(\"GloVe \",embedding_dim, ' loaded!')\n",
        "\n",
        "    embedding_layer = Embedding(vocab_len, embedding_dim, mask_zero=False, trainable=False) # we have a limited vocab so we \n",
        "                                                                                            # do not train the embedding layer\n",
        "                                                                                            # we use 0 as padding so => mask_zero=True\n",
        "    embedding_layer.build((None,))\n",
        "    embedding_layer.set_weights([embedding_matrix])\n",
        "    return embedding_layer\n",
        "\n",
        "embeddings_encoder = make_embedding_layer(encoder_vocab_size, wordtoix_enc, embedding_dim=embedding_dim, glove=True)\n",
        "embeddings_decoder = make_embedding_layer(decoder_vocab_size, wordtoix_dec, embedding_dim=embedding_dim, glove=True)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading glove...\n",
            "GloVe  200  loaded!\n",
            "Loading glove...\n",
            "GloVe  200  loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1e7yinOWwbV"
      },
      "source": [
        "#### transformer layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ0o2dGPWwbW"
      },
      "source": [
        "# hyper-params\n",
        "init_lr = 1e-3\n",
        "lmbda_l2 = 0.1\n",
        "d_out_rate = 0.1 \n",
        "num_layers = 1 \n",
        "d_model = embedding_dim # d_model is the representation dimension or embedding dimension of a word \n",
        "dff = 9000 # number of neurons in feed forward network\n",
        "num_heads = 5 "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtHhPUjUWwbW"
      },
      "source": [
        "# The Point-wise feed-forward network block is essentially a \n",
        "# two-layer linear transformation which is used identically throughout the model\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return krs.Sequential([\n",
        "        krs.layers.Dense(dff, activation='relu',kernel_regularizer=krs.regularizers.l2(l=lmbda_l2)),\n",
        "        krs.layers.Dense(d_model,kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "    ])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WViuD-4wWwbW",
        "scrolled": true
      },
      "source": [
        "class EncoderLayer(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=d_out_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = krs.layers.Dropout(rate)\n",
        "        self.dropout2 = krs.layers.Dropout(rate)\n",
        "   \n",
        "    # it has 1 layer of multi-headed attention\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        # Fourier Based Attention\n",
        "        attn_output = fftn(x)\n",
        "\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        #out1 = self.layernorm1(x + attn_output)\n",
        "        out1 = self.layernorm1(attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        out2 = fftn(out2)\n",
        "        return out2"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxXnmVvbWwbW"
      },
      "source": [
        "class DecoderLayer(krs.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=d_out_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        \n",
        "        # d_model: embedding dimension of each word\n",
        "        self.mha2 = FourierAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = krs.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = krs.layers.Dropout(rate)\n",
        "        self.dropout2 = krs.layers.Dropout(rate)\n",
        "        self.dropout3 = krs.layers.Dropout(rate)\n",
        "    \n",
        "    def pad_second_dim(input, desired_size):\n",
        "        padding = tf.tile([[0]], tf.stack([tf.shape(input)[0], desired_size - tf.shape(input)[1]], 0))\n",
        "        return tf.concat([input, padding], 1)\n",
        "\n",
        "    # it has 2 layers of multi-headed attention\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        \n",
        "        # Fourier Based Attention\n",
        "        fourier = fftn(x)\n",
        "\n",
        "        attn1 = self.dropout1(fourier, training=training)\n",
        "        #out1 = self.layernorm1(attn1 + x)\n",
        "        out1 = self.layernorm1(attn1)\n",
        "\n",
        "        fourier = fftn(enc_output)\n",
        "        attn2 = self.mha2(fourier, out1, padding_mask)\n",
        "\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        #out2 = self.layernorm2(attn2 + out1) \n",
        "        out2 = self.layernorm2(attn2)\n",
        "\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "        \n",
        "        return out3,enc_output"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnIwAcmTWwbX"
      },
      "source": [
        "class Encoder(krs.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=d_out_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = embeddings_encoder\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = krs.layers.Dropout(rate)\n",
        "        self.dropout_embd = krs.layers.Dropout(rate)\n",
        "        \n",
        "    def call(self, x, training, mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout_embd(x, training=training) # dropout added to encoder input changed from nothing to this\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "    \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "        return x"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPRW0wt2WwbX"
      },
      "source": [
        "class Decoder(krs.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=d_out_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = embeddings_decoder\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] # a list of decoder layers\n",
        "        self.dropout = krs.layers.Dropout(rate)\n",
        "    \n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x,enc_output = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask) \n",
        "            \n",
        "        return x,enc_output"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIiM0_8zWwbX"
      },
      "source": [
        "#### Final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcBSkz-bWwbY"
      },
      "source": [
        "class Transformer(krs.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "                                     target_vocab_size, pe_input, pe_target, rate=d_out_rate):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = krs.layers.Dense(target_vocab_size, kernel_regularizer=krs.regularizers.l2(l=lmbda_l2))\n",
        "        \n",
        "        \n",
        "    # training argument is used in dropout inputs\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
        "       \n",
        "        #dec_output, attention_weights, att = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "        dec_output,enc_output = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        \n",
        "        return final_output\n",
        "        #return final_output, attention_weights, att"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOFGDYoIWwbY"
      },
      "source": [
        "transformer = Transformer(\n",
        "    num_layers, \n",
        "    d_model, \n",
        "    num_heads, \n",
        "    dff,\n",
        "    encoder_vocab_size, \n",
        "    decoder_vocab_size, \n",
        "    pe_input=max_len_news,\n",
        "    pe_target=max_len_summary,\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psH7lJqlWwbY"
      },
      "source": [
        "#### Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deSw59pSWwbY"
      },
      "source": [
        "# Padding mask for masking \"pad\" sequences so \n",
        "# they won't affect the loss\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "# Lookahead mask for masking future words from\n",
        "# contributing in prediction of current words in self attention\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl_blh1vWwbY"
      },
      "source": [
        "# this function is use in training step\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "        \n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd6gf1V_WwbZ"
      },
      "source": [
        "#### training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqtVJJRsWwbZ"
      },
      "source": [
        "lr_schedule = krs.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=init_lr, # originally was 1e-5\n",
        "    decay_steps=4000, # approximately 5 epochs\n",
        "    decay_rate=0.95) # originally was 0.9"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n21i51VoWwbZ"
      },
      "source": [
        "\n",
        "optimizer2 = Adam(lr_schedule , beta_1=0.9, beta_2=0.98, epsilon=1e-9) # changed to init\n",
        "loss_object = SparseCategoricalCrossentropy(from_logits=True, reduction='none') # added softmax changed from_logits to false"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POZbKMs-WwbZ"
      },
      "source": [
        "def loss_function(real, pred, l2= False):\n",
        " \n",
        "    if l2:\n",
        "        lambda_ = 0.0001\n",
        "        l2_norms = [tf.nn.l2_loss(v) for v in transformer.trainable_variables]\n",
        "        l2_norm = tf.reduce_sum(l2_norms)\n",
        "        l2_value = lambda_ * l2_norm\n",
        "        loss_ = loss_object(real, pred) + l2_value\n",
        "    else:\n",
        "        loss_ = loss_object(real, pred) \n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq8uzNTIWwba"
      },
      "source": [
        "checkpoint_path4 =\"checkpoints4\"\n",
        "\n",
        "ckpt4 = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer2)\n",
        "\n",
        "ckpt_manager4 = tf.train.CheckpointManager(ckpt4, checkpoint_path4, max_to_keep=100)\n",
        "\n",
        "#if ckpt_manager4.latest_checkpoint:\n",
        "#     ckpt4.restore(ckpt_manager4.latest_checkpoint)\n",
        "#     print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fCqpYLKWwba"
      },
      "source": [
        "#### inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMGbB-ikWwba"
      },
      "source": [
        "def evaluate(input_document):\n",
        "    input_document = document_tokenizer.texts_to_sequences([input_document])\n",
        "    input_document = krs.preprocessing.sequence.pad_sequences(input_document, maxlen=max_len_news, \n",
        "                                                                           padding='post', truncating='post')\n",
        "    \n",
        "    encoder_input = tf.expand_dims(input_document[0], 0)\n",
        "\n",
        "    decoder_input = [summary_tokenizer.word_index[start_token]]\n",
        "    output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "    for i in range(max_len_summary):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "\n",
        "        #predictions, attention_weights, att = transformer(\n",
        "        predictions = transformer(\n",
        "            encoder_input, \n",
        "            output,\n",
        "            False,\n",
        "            enc_padding_mask,\n",
        "            combined_mask,\n",
        "            dec_padding_mask\n",
        "        )\n",
        "\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "        # stop prediciting if it reached end_token\n",
        "        if predicted_id == summary_tokenizer.word_index[end_token]:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "    return tf.squeeze(output, axis=0)\n",
        "\n",
        "def summarize(input_document):\n",
        "    summarized = evaluate(input_document=input_document).numpy()\n",
        "    summarized = np.expand_dims(summarized[1:], 0)  # remove start_token\n",
        "    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pm7RdxY0Wwba",
        "scrolled": true
      },
      "source": [
        "def validate():\n",
        "    print('validation started ...')\n",
        "    val_loss.reset_states()\n",
        "    for (batch, (inp, tar)) in enumerate(dataset_val):    \n",
        "        tar_inp = tar[:, :-1] \n",
        "        tar_real = tar[:, 1:] \n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "        # Operations are recorded if they are executed within this context manager\n",
        "        # and at least one of their inputs is being \"watched\". Trainable variables are automatically watched\n",
        "        predictions = transformer(\n",
        "            inp, tar_inp, \n",
        "            False, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        val_loss(loss)\n",
        "    print('\\n* Validation loss: {} '.format(val_loss.result()) )\n",
        "    return val_loss.result()\n",
        "# validate()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raTDvNqeWwba"
      },
      "source": [
        "@tf.function # Compiles a function into a callable TensorFlow graph\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1] # <startseq> hi this is andrew\n",
        "    tar_real = tar[:, 1:] # hi this is andrew <endseq>\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "    \n",
        "    # Operations are recorded if they are executed within this context manager\n",
        "    # and at least one of their inputs is being \"watched\". Trainable variables are automatically watched\n",
        "    with tf.GradientTape() as tape:\n",
        "        #predictions, _, att = transformer(\n",
        "        predictions  = transformer(\n",
        "            inp, tar_inp, # input, target\n",
        "            True, \n",
        "            enc_padding_mask, \n",
        "            combined_mask, \n",
        "            dec_padding_mask\n",
        "        )\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "        \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer2.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    # mean the loss with new computed  loss of the step\n",
        "    train_loss(loss)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPAUKy2Wwbb"
      },
      "source": [
        "history={'val':[],'train':[]}\n",
        "EPOCHS = 500\n",
        "not_progressing = 0\n",
        "# Computes the (weighted) mean of the given loss values.\n",
        "train_loss = krs.metrics.Mean(name='train_loss')\n",
        "val_loss = krs.metrics.Mean(name='val_loss')"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqxIhEkbWwbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc92752-525d-4fdd-e72b-c4faf409ba68"
      },
      "source": [
        "params = {\n",
        "'lmbda_l2' : lmbda_l2,\n",
        "'d_out_rate' :d_out_rate,\n",
        "'num_layers' : num_layers ,\n",
        "'d_model' : d_model  ,\n",
        "'dff' : dff ,\n",
        "'num_heads' : num_heads,\n",
        "'init_lr':init_lr}\n",
        "params"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'d_model': 200,\n",
              " 'd_out_rate': 0.1,\n",
              " 'dff': 9000,\n",
              " 'init_lr': 0.001,\n",
              " 'lmbda_l2': 0.1,\n",
              " 'num_heads': 5,\n",
              " 'num_layers': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNKhNr8GWwbb"
      },
      "source": [
        "ep = 1\n",
        "best_val_loss = np.inf\n",
        "i1,i2,i3,i4 = np.random.randint(len(summaries_val)),np.random.randint(len(summaries_val)),np.random.randint(len(summaries_val)),np.random.randint(len(summaries_val))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu_4SmaDWwbb",
        "scrolled": true
      },
      "source": [
        "print(params)\n",
        "print('#'*40)\n",
        "\n",
        "rouge = Rouge()\n",
        "scores = []\n",
        "for epoch in range(ep,EPOCHS+1):\n",
        "    ep = epoch\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "  \n",
        "    for (batch, (inp, tar)) in enumerate(dataset):  # (input, target) tuple\n",
        "        \n",
        "        train_step(inp, tar)\n",
        "    \n",
        "        if batch % 150 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch , batch, train_loss.result()))\n",
        "                  \n",
        "    print()\n",
        " \n",
        "    print(summarize(clean_words(longreview_train['long'][i1])))\n",
        "    #print(summarize(clean_words(longreview_val['long'][i2])))\n",
        "    #print(summarize(clean_words(longreview_val['long'][i3])))\n",
        "    #print(summarize(clean_words(longreview_val['long'][i4])))\n",
        "    print()\n",
        "    \n",
        "    # store rouge scores in list\n",
        "    try:\n",
        "      reference = clean_words(summaries_val['short'][100])\n",
        "      predict = summarize(clean_words(longreview_train['long'][100]))\n",
        "      score = rouge.get_scores(predict, reference)\n",
        "      scores.append(score)\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "    val_loss_ = validate().numpy()\n",
        "    history['val'].append((epoch,val_loss_))\n",
        "    print ('\\n* Train Loss {:.4f}'.format(train_loss.result()))\n",
        "    history['train'].append((epoch,train_loss.result().numpy()))\n",
        "    \n",
        "    \n",
        "    if best_val_loss-val_loss_ > 0.1:\n",
        "        ckpt_save_path4 = ckpt_manager4.save()\n",
        "        print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path4))  \n",
        "        best_val_loss = val_loss_\n",
        "    \n",
        "    hist(history)\n",
        "    print('Current Lr: ',optimizer2._decayed_lr('float32').numpy())\n",
        "    print ('\\nTime taken for this epoch: {:.2f} secs\\n'.format(time.time() - start))\n",
        "    print('='*40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BvblFJWwbc"
      },
      "source": [
        "hist(history)\n",
        "params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zotXoeXyGPrM"
      },
      "source": [
        "import pickle\n",
        "with open('scores.pkl', 'wb') as f:\n",
        "    pickle.dump(scores, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dXxvU9YRtul"
      },
      "source": [
        "# **Plot Rouge Scores per Epoch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttzbCy4DGrMc"
      },
      "source": [
        "f_number = scores \n",
        "f_number = [x[0]['rouge-2']['f'] for x in scores]\n",
        "plt.plot(f_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqwMLpIhWwbc"
      },
      "source": [
        "you can use more data or more regularization to avoid the overfitting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WghHS4dYWwbd"
      },
      "source": [
        "#print(clean_words(longreview_val['long'][i2]))\n",
        "print()\n",
        "print(summarize(clean_words(longreview_val['long'][i2])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm0pGl6mLiWg"
      },
      "source": [
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(hypothesis, reference)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoN-pqYKMuoU"
      },
      "source": [
        "scores[0]['rouge-2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykPEOBh8Z6wx"
      },
      "source": [
        "len(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbhwaijBQ2Zb"
      },
      "source": [
        "f_number = scores[0]['rouge-2']['f']\n",
        "f_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BepVTuV_eRwM"
      },
      "source": [
        "# **Calculate Scores**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fng3c9gvEryx"
      },
      "source": [
        "!pip install -q tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou2fdi91EvgW"
      },
      "source": [
        "import tensorflow_text as text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m7Dl52iEzPg"
      },
      "source": [
        "result = text.metrics.rouge_l(origin, result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}